# История формата

ORC: Optimized RC Format (где RC = Record Columnar), собственно базируется на своем предшественнике (RC Format-е) и 
всего-лишь

`the smallest, fastest columnar storage for Hadoop workloads`
  
(это цитата с главной страницы проекта - https://orc.apache.org/. Немного погрузившись - я ей верю...)

Формат уже не юн - создан в 2013 году, сейчас стабильная версия - v1, в работе - v2. 
Достаточно хорошо документирован - все, о чем я здесь пишу, базируется на спецификации 
(https://orc.apache.org/specification/) и проверено питновским кодом (который здесь же).

# Верхнеуровневое описание формата

ORC файл полностью самодостаточен (как и любой другой формат Hadoop - иначе как читать, никаких отдельных метаданных нет...). 
Файл состоит из достаточно больших "страйпов" (stripes) порядка 64МБ каждый и "хвостовичка", содержащего общую для всего файла метаинформацию. Страйп является также самодостаточным (в плане данных) и содержит подмножество строк исходной таблицы. Для чтения данных (подмножества строк) достаточно прочитать хвостовик файла (мета информацию) и страйп(ы), который содержит данные нужных строк.

Формат ориентирован на хранение колонок - в пределах страйпа можно прочитать данные каждой из колонок независимо.

Каждая часть файла (про "части" - ниже) может быть сжата (ZLIB, например), при этом опять же сжатие не должно мешать возможности считать части файла по отдельности.

Файл содержит индексы (на трех уровнях: файл, страйп и блок строк, про блоки - ниже). Что дополнительно ускоряет чтение данных из файла.

Значения колонок хранятся в encoded виде (например, RLE), причем виды кодировки для одной колонки в пределах одного блока строк могут быть разными - все зависит от данных. Кстати, именно этим объясняется тот размер (628 байт), который был приведен выше: данные "легли" так удачно, что RLE кодировка их ну совсем сжала... См. детали ниже.

Ну и в завершение - метаданные хранятся с использованием Protocol Buffers (https://developers.google.com/protocol-buffers/docs/encoding), что тоже прикольно.

# Какая от этого конкретная польза и как ее получить

Формат очень хорошо поддерживается Hadoop-ом, HIVE-у достаточно просто указать, что таблица будет храниться как ORC. Тогда данные таблицы будут храниться в ORC файлах (с ZLIB сжатием по умолчанию).

HIVE имеет утилиту просмотра метаданных ORC файлов, т.е. их можно поизучать (я изучал прямо на питоне по спецификации).

SQOOP пока не поддерживает ORC (говорят, что будет). Поэтому для создания ORC таблиц приходится делать 2 шага

* sqoop таблицу из внешней базы в текстовый файл
* hive создает таблицу с данными из текстового файла

Абсолютные времена последней операции ("перегона" данных из текста в ORC) вполне приемлемы.

Кроме того, можно воспользоваться C++ библиотекой (см. приведенные выше сайт проекта - там есть описание, как собраться С++ библиотеку, я ее собрал - работает) и создавать или читать файлы напрямую из C++.

Сравнение форматов (parquet vs orc) - в процессе. Пока по результатам изучения "ставка" сделана на ORC (хотя в планах изучить parquet также, как и ORC - в байтах...)

# Собственно описание деталей

Вот тут я чуть призадумался - не повторять же здесь спецификацию, в ней все хорошо написано (просто надо внимательно читать).

Наверное вот как лучше сделать: дальнейшее чтение лучше делать в "юпитеровской книжке".

А здесь (ниже) я постараюсь описать то, что "не влезло" непосредственно в книжку (даже интересно - что это будет...).

# Что не влезло в книжку

## Сжатие

`If the ORC file writer selects a generic compression codec (zlib or snappy), every part of the ORC file except for the Postscript is compressed with that codec. However, one of the requirements for ORC is that the reader be able to skip over compressed bytes without decompressing the entire stream. To manage this, ORC writes compressed streams in chunks with headers as in the figure below. To handle uncompressable data, if the compressed data is larger than the original, the original is stored and the isOriginal flag is set. Each header is 3 bytes long with (compressedLength * 2 + isOriginal) stored as a little endian value. For example, the header for a chunk that compressed to 100,000 bytes would be [0x40, 0x0d, 0x03]. The header for 5 bytes that did not compress would be [0x0b, 0x00, 0x00]. Each compression chunk is compressed independently so that as long as a decompressor starts at the top of a header, it can start decompressing without the previous bytes.`

В-общем это надо понимать так:

* если в файле применяется сжатие, то все метаданные (см. структуру ниже) будут сжаты. Про сами данные не понял до конца - сжаты они или нет (в этом примере я сжатие отключил - может быть, попозже обновлю пост, либо напишу отдельно: есть мысль посмотреть, как устроены реальные данные - не из тестового примера, а из базы)
* про каждый блок метаданных известна его длина (и смещение, которое получается обратным счетом - см. книжку)
* тогда нужно отступить 3 байта от начала сжатых данных секции (именно это и хранится в переменной comprOffs - смещение относительно начала секции. Для несжатых файлов это всегда 0)
* считанные "длина секции - 3" байта нужно "декомпрессить" так, как это написано в книжке (zlib deflate)

Не сложно... Но пока я подобрал "все ключи" - ручек в стены полетало...

## Общая структура файла

Все же она недостаточно просто описана в спецификации, ниже приведу свою "версию" описания структуры.

ORC файл состоит из двух логических частей

* повторяющиеся страйпы
* хвостовик (который я упоминал выше)

ORC файл читается

* снизу-вверх (хвостовик)
* сверху вниз (страйпы), точнее - они адресуются по абсолютному смещению в файле и имеют размер (все это хранится в хвостовике)

    *футер страйпа находится в его конце, сначала читается он

Попробую обойтись без рисунка - списком секций (в скобках - длина). Не расписывал все поля - только те, которые позволяют читать файл.

    Повтояющиеся страйпы
        Страйп(ы)
              их размеры известны из хвостовика
              структура страйпа описана в книжке
    Хвостовик
        MetaData, может быть сжать (PostScript.metaDataLength байт)
            содержит информацию о страйпах (сколько строк в каждом, мин макс и сумма каждой колонки), это не позволяет адресоваться по файлу, но позволяет понять, какие страйпы нужно читать
        Footer, может быть сжат (PostScript.footerLength байт)
            содержит количество страйпов (неявно - количество секций)
            про каждый страйп - смещение и длина (длина в разбивке по составным страйпа)
        PostScript, никогда не сжат, гарантированно не может быть длиннее 256 байт (ps байт)
            содержит длину Footer (поле footerLength)
            содержит длину Metadata (поле metadataLength)
        ps: длина PostScript (1 байт)
 
