{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Объединение dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Структура шага\n",
    "\n",
    "* аудио по join и union\n",
    "    * под него слайды Добавление строк и/или колонок, Join() и их виды, Метод join() и его параметры, Union() и его использование\n",
    "* аудио про lazy evaluation\n",
    "    * под него слайд Lazy evaluation и оптимизация\n",
    "* все остальное - материалы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление строк и/или колонок\n",
    "\n",
    "* всегда два участника\n",
    "* join() - \"джойнит\" один датафрейм с другим\n",
    "* union() - конкатенирует один датафрейм с другим"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join() и их виды\n",
    "\n",
    "* Inner joins: остаются строки, которые есть в левом и правом датафреймах\n",
    "* Outer joins: остаются строки, ключи которых есть в левом или правом датафрейме\n",
    "* Left outer joins: остаются строки, ключи которых есть в левом датафрейме\n",
    "* Right outer joins: остаются строки, ключи которых есть в правом датафрейме\n",
    "* Left semi joins: остаются только строки левого датафрейма, ключи которых есть в правом датафрейме\n",
    "* Left anti joins: остаются только строки левого датафрейма, ключей которых нет в правом датафрейме\n",
    "* Natural joins: выполняет объединение с помощью неявного сопоставления колонок в двух датафреймах (сопоставление по именам)\n",
    "* Cross (or Cartesian) joins: каждая строка левого датафрейма объединяется с каждой строкой правого датафрейма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод join() и его параметры\n",
    "\n",
    "* leftDf.join(rightDf,joinExpression,how)\n",
    "* how: строка (inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, left_anti)\n",
    "* joinExpression: выражение (например `person[\"graduate_program\"]==graduateProgram['id']`)\n",
    "\n",
    "В материалах разбере виды join на примерах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union() и его использование\n",
    "\n",
    "* firstDf.union(secondDf)\n",
    "* конкатенация данных двух датафреймов\n",
    "* работает по \"локации\" (не по схеме)\n",
    "* максимальная аккуратность, расположение колонок в нужном порядке\n",
    "* unionByName() работает по именам колонок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy evaluation и оптимизация\n",
    "\n",
    "* трансформации - это правило формирования dataframe\n",
    "* \"промежуточные\" датафреймы в цепочке преобразований не снижают ее эффективность\n",
    "* \"многостраничные\" select-ы появляются из-за стремления к повышению эффективности SQL запроса\n",
    "* используя spark мы можем \"многостраничный\" select выразить последовательностью простых транформаций\n",
    "* повышается \"читабельность\" и наглядность\n",
    "* inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_antiвозможность более \"гибкого\" управления кэшированием промежуточных результатов\n",
    "\n",
    "(см. примеры в материалах)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/mk/mk_win/projects/SparkEdu/lib/python3.5/site-packages/pyspark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = \"local\"\n",
    "#master = \"yarn\"\n",
    "spark = SparkSession.builder.master(master).appName(\"spark_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Специально подобранные данные**\n",
    "\n",
    "Для того, чтобы увидеть разницу, нужно воспользоваться специально подобранными данными (иначе просто потеряться). \n",
    "\n",
    "Данные взяты из книги - авторы постарались..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "(0, \"Bill Chambers\", 0, [100]),\n",
    "(1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "(2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    ".toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "(0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "(2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "(1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    ".toDF(\"id\", \"degree\", \"department\", \"school\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n",
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.show()\n",
    "graduateProgram.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выражение, по которому будет происходить join, всегда одинаково."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"inner\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outer**\n",
    "\n",
    "появляются строки из правого фрейма, для которых нет ключей в левом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left outer**\n",
    "\n",
    "поменяли местами датафреймы, теперь все строки из левого датафрейма показаны, не для всех нашлись ключи в правом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Right outer**\n",
    "\n",
    "ключи из правого датафрейма, не для всех нашлось соответствие в левом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left semi**\n",
    "\n",
    "опять поменяли местами датафреймы. Остались только те программы (левый датафрейм), для которых были соответствия в правом. Правого датафрейма нет (т.е. чисто работа со строками одного датафрейма)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_semi\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left anti**\n",
    "\n",
    "опять поменяли местами датафреймы. Остались только те программы (левый датафрейм), для которых не было соответствия в правом. Правого датафрейма нет (т.е. чисто работа со строками одного датафрейма)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_anti\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры union()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просто добавим датафрейм в \"хвост\" себе же"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram.union(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменяем местами колонки в добавляемом датафрейме - увидим, какая \"каша\" получилась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+-----------+\n",
      "|     id| degree|          department|     school|\n",
      "+-------+-------+--------------------+-----------+\n",
      "|      0|Masters|School of Informa...|UC Berkeley|\n",
      "|      2|Masters|                EECS|UC Berkeley|\n",
      "|      1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|Masters|      0|School of Informa...|UC Berkeley|\n",
      "|Masters|      2|                EECS|UC Berkeley|\n",
      "|  Ph.D.|      1|                EECS|UC Berkeley|\n",
      "+-------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram.union(graduateProgram.select(\"degree\",\"id\",\"department\",\"school\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если воспользоваться методом `unionByName`, то все будет корректно даже с переставленными колонками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram.unionByName(graduateProgram.select(\"degree\",\"id\",\"department\",\"school\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многостраничный select и spark\n",
    "\n",
    "В этой части разберем на примере - как использование spark снижает требования к SQL и позволяет создавать более модульные, \"читабельные\" и потенциально более эффективные программы.\n",
    "\n",
    "Код, приведенный ниже, нельзя исполнить (нет таблиц), его можно просто посмотреть (поверив, что он работает).\n",
    "\n",
    "Возьмем относительно простой select: \n",
    "\n",
    "`select\n",
    "    con.contract_id as contract_id,\n",
    "    con.date_insert as date_insert,\n",
    "    con.date_sign as date_sign,\n",
    "    con.begin_date as begin_date,\n",
    "    con.end_date as end_date,\n",
    "    con.product_id as product_id,\n",
    "    con.channel_sale_id as channel_sale_id,\n",
    "    con.contract_prev_id as contract_prev_id,\n",
    "    con.contract_option_id as contract_option_id,\n",
    "    con.owner_subject_id as owner_subject_id,\n",
    "    con.contract_type_id as contract_type_id,\n",
    "    cst.contract_status_type_id as status_type_id,\n",
    "    sbj.subject_type_id as subject_type_id,\n",
    "    sbj.subject_name as subject_name,\n",
    "    pp.birth_date as birth_date,\n",
    "    uv.hid_party as hid_party\n",
    "from\n",
    "    kasko.contract con\n",
    "    join kasko.contract_status cst on cst.contract_status_id = con.contract_status_id\n",
    "    join kasko.subject sbj on sbj.subject_id = con.owner_subject_id\n",
    "    left join kasko.physical_person pp on pp.subject_id = con.owner_subject_id\n",
    "    left join kasko.united_view uv on uv.source_id = sbj.subject_id and uv.source_name = 'UNI'\n",
    "where\n",
    "    department_id=3075`\n",
    "\n",
    "И посмотрим, на какие \"кирпичики\" его можно разложить в spark.\n",
    "\n",
    "Видно, что участвуют 5 таблиц. Начнем с того, что сформулируем, что мы хотим получить от каждой таблицы (воспользуемся синтаксисом SQL для простоты - про него чуть позже в этом модуле)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# таблица contract\n",
    "qCon = \"\"\"\n",
    "select\n",
    "    contract_id as con_contract_id,\n",
    "    date_insert as con_date_insert,\n",
    "    date_sign as con_date_sign,\n",
    "    begin_date as con_begin_date,\n",
    "    end_date as con_end_date,\n",
    "    product_id as con_product_id,\n",
    "    channel_sale_id as con_channel_sale_id,\n",
    "    contract_prev_id as con_contract_prev_id,\n",
    "    contract_option_id as con_contract_option_id,\n",
    "    owner_subject_id as con_owner_subject_id,\n",
    "    contract_status_id as con_contract_status_id,\n",
    "    contract_type_id as con_contract_type_id\n",
    "from\n",
    "    kasko.contract\n",
    "where\n",
    "    department_id = 3075\n",
    "\"\"\"\n",
    "dfCon = sp.sql(qCon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# таблица contract_status\n",
    "qCStat = \"\"\"\n",
    "select \n",
    "    contract_status_id as cst_status_id,\n",
    "    contract_status_type_id as cst_status_type_id\n",
    "from\n",
    "    kasko.contract_status\n",
    "\"\"\"\n",
    "dfCStat = sp.sql(qCStat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# таблица subject\n",
    "qSubj = \"\"\"\n",
    "select\n",
    "    subject_id as sbj_subject_id,\n",
    "    subject_type_id as sbj_subject_type_id,\n",
    "    subject_name as sbj_subject_name\n",
    "from\n",
    "    kasko.subject\n",
    "\"\"\"\n",
    "dfSubj = sp.sql(qSubj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# таблица physical_person\n",
    "qPPers = \"\"\"\n",
    "select\n",
    "    subject_id as pp_subject_id,\n",
    "    birth_date as pp_birth_date\n",
    "from\n",
    "    kasko.physical_person\n",
    "\"\"\"\n",
    "dfPPers = sp.sql(qPPers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# таблица united_view\n",
    "qUView = \"\"\"\n",
    "select\n",
    "    source_id as uv_source_id,\n",
    "    hid_party as uv_hid_party\n",
    "from\n",
    "    kasko.united_view\n",
    "where\n",
    "    source_name = 'UNI'\n",
    "\"\"\"\n",
    "dfUView = sp.sql(qUView)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все эти датафреймы нам могут понадобиться многократно (в нашем конвейере преобразований данных), можно включить в них побольше колонок (на все случаи жизни). Получим \"универсальные\" кирпичики, из которых потом будем складывать \"запросы\" (трансформации).\n",
    "\n",
    "Далее формулируем правила связывания датафреймов, это всего лишь выражения, которые показывают - как связаны между собой таблицы. Эти знания универсальны, могут быть унаследованы из схемы реляционной базы данных. Тоже универсальные \"кирпичики\" другого рода - связи между таблицами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_stat = f.col(\"cst_status_id\")==f.col(\"con_contract_status_id\")\n",
    "con_subj_own = f.col(\"con_owner_subject_id\")==f.col(\"sbj_subject_id\")\n",
    "con_ppers_own = f.col(\"con_owner_subject_id\")==f.col(\"pp_subject_id\")\n",
    "subj_uview = f.col(\"sbj_subject_id\")==f.col(\"uv_source_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И, наконец, формулируем наш \"многостраничный\" запрос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resDf = dfCon.join(dfSubj,con_subj_own,\"inner\")\\\n",
    "    .join(dfCStat,con_stat, \"inner\")\\\n",
    "    .join(dfPPers,con_ppers_own, \"left\")\\\n",
    "    .join(dfUView,subj_uview,\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Он эквивалентен исходному `select`-у, но... проще (может быть, непривычнее, но проще). Он состоит из кирпичиков, которые объединяются в простую цепочку. Оптимизатор spark сделает свое дело - итоговый запрос будет выполнен оптимально (также, как оптимально будет выполнен и исходный SQL запрос).\n",
    "\n",
    "А теперь представим себе, что таблица `contract` у нас участвует в половине преобразований. Понять из набора select-ов, какой набор колонок нужно выбирать, какую таблицу кэшировать сложнее, чем в цепочках spark, потому что они набираются из \"кирпичиков\", каждый из которых фактически представляет собой подмножество полей исходных таблиц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
