{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD: Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что такое RDD\n",
    "\n",
    "* набор объектов, разбитых на разделы (`partitions`)\n",
    "* часть Low Level API\n",
    "* dataframe \"компилируются\" в RDD\n",
    "* менее функционально полная абстракция spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем нам нужны \n",
    "\n",
    "* для понимания концепции (как SQL)\n",
    "* работы с legacy кодом\n",
    "* если нужна функциональность, которой нет в Structured API (например, управление размещением данных по разделам)\n",
    "* фокус все равно на Structured API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формальное определение RDD\n",
    "\n",
    "Процитирую из книги - RDD внутренне характеризуется следующим набором атрибутов:\n",
    "\n",
    "* списком разделов (`partitions`)\n",
    "* фукнцией для вычисления каждого `split`\n",
    "* списком зависимостей от других RDD\n",
    "* (опционально) функцией `Partitioner` для `key-value RDD`\n",
    "* (опционально) списком предпочтений узлов, на которых обрабатывать каждый `split` (например, список расположений блоков файла HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трансформации и действия\n",
    "\n",
    "* та же модель, что и с dataframe\n",
    "* набор трансформаций меньше (и он - другой)\n",
    "* используется lazy evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание RDD\n",
    "\n",
    "* `spark.sparkConect` - точка входа в Low Level API\n",
    "* `parallelize()` - создание RDD\n",
    "* `textFile()` - создание RDD из файлов (каждая строка - элемент RDD)\n",
    "* `wholeTextFiles()` - каждый файл - элемент RDD (например, наши сложные XML или JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные трансформации\n",
    "\n",
    "* `distinct()` - оставить только уникальные элементы RDD\n",
    "* `filter()` - отфильтровать элементы с помощью заданной функции\n",
    "* `map()` - преобразовать элементы с помощью заданной функции\n",
    "* `flatMap()` - преобразовать с добавлением элементов RDD\n",
    "* `sort()` - упорядочить элементы RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные действия\n",
    "\n",
    "* `reduce()` - сворачивает элементы RDD, используя коммутативную и ассоциативную функцию (с двумя параметрами)\n",
    "* `count()` - посчитать количество элементов в RDD\n",
    "* `first()` - собрать первые N элементов RDD на драйвер\n",
    "* `min()/max()` - найти максимум и минимум \n",
    "* `collect()/take()` - собрать/собрать N элементов RDD на драйвер\n",
    "* `saveAsTextFile()` - сохранить RDD в виде текстового файла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python и RDD\n",
    "\n",
    "При использовании python-а и RDD происходит потеря производительности (в отличие от RDD + java или использования Structured API): RDD - это как использование python UDF для каждого элемента RDD\n",
    "\n",
    "* сначала данные сериализуются в python структуры данных\n",
    "* обрабатываем их на python-е\n",
    "* сериализуем данные в формат JVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практикуемся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/mk/mk_win/projects/SparkEdu/lib/python3.5/site-packages/pyspark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"spark_app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создание RDD**\n",
    "\n",
    "Давайте создадим простейший RDD - слова файла (как строки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrds = spark.sparkContext.parallelize(\"this is spark and it is great\".split(),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Из файла**\n",
    "\n",
    "Прочитаем файл и создадим RDD со строками файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Country,Region,Population,Area (sq. mi.),Pop. Density (per sq. mi.),Coastline (coast/area ratio),Net migration,Infant mortality (per 1000 births),GDP ($ per capita),Literacy (%),Phones (per 1000),Arable (%),Crops (%),Other (%),Climate,Birthrate,Deathrate,Agriculture,Industry,Service'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.sparkContext.textFile(\"data/countries of the world.csv\")\n",
    "lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Трансформации**\n",
    "\n",
    "Начнем с фильтрации, давайте сделаем что-то со словами, ниже - разберем, что получилось и почему RDD не эффективны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def startsWithS(individual):\n",
    "    return individual.startswith(\"s\")\n",
    "\n",
    "wrds.filter(lambda word: startsWithS(word)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обсуждение сделанного.\n",
    "\n",
    "Что произошло:\n",
    "\n",
    "* мы написали функцию на python (startsWithS)\n",
    "* она была spark-ом как-то переведена на java (executor-ы \"понимают\" только java)\n",
    "* на каждом узле кластера (где \"живут\" разделы нашего RDD) для каждого элемента раздела RDD была вызвана эта функция\n",
    "* на вход был подан \"переведенный\" в python строку (десериализация) элемент раздела RDD с этого узла\n",
    "* функция отработала, вернула True или False\n",
    "* в раздел нового RDD попала часть элементов старого (возможно, произошла еще одна сериализация)\n",
    "* все разделы собираются на драйвер (`collect`) и происходит еще одна десериализация (в типы python)\n",
    "\n",
    "Обратите внимание - результат `collect()` - список строк (чисто python объект), т.е. никакого преобразования в объекты spark не происходит. В функции `startsWithS()` мы работаем со строками. Spark делает за нас все преобразования, но это снижает эффективность, нужно иметь это в виду"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dictinct()**\n",
    "\n",
    "У нас есть повторяющиеся слова - удалим их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'and', 'is', 'spark', 'this', 'great']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**map()**\n",
    "\n",
    "Преобразуем наши слова в пары - слово, длина"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 4),\n",
       " ('is', 2),\n",
       " ('spark', 5),\n",
       " ('and', 3),\n",
       " ('it', 2),\n",
       " ('is', 2),\n",
       " ('great', 5)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getLen(individual):\n",
    "    return (individual,len(individual))\n",
    "\n",
    "wrds.map(getLen).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flatMap()**\n",
    "\n",
    "Разобъем слова по символам (т.е. увеличим количество разделов - каждый символ = раздел) и соберем на `driver` первые 6 разделов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'i', 's', 'i', 's']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def toChars(individual):\n",
    "    return list(individual)\n",
    "\n",
    "wrds.flatMap(toChars).take(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sort()**\n",
    "\n",
    "Давайте упорядочим слова по их длине (по убыванию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark', 'great', 'this', 'and', 'is', 'it', 'is']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds.sortBy(lambda word: len(word) * -1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Действия**\n",
    "\n",
    "мы уже видели `collect()/take()`, давайте посмотрим `reduce()`: оставим самое длинное слово из нашего набора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wordLengthReducer(leftWord, rightWord):\n",
    "    if len(leftWord) > len(rightWord):\n",
    "        return leftWord\n",
    "    else:\n",
    "        return rightWord\n",
    "\n",
    "wrds.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**count()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**saveAsTextFile()**\n",
    "\n",
    "Сохраним наши слова в виде \"файла\" - как обычно для Spark: будет создана директория, в которую будет записан RDD, каждый раздел - отдельный файл. Количество разделов мы тоже выведем, хотя это - 3 (мы именно так создавали RDD - см. выше)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm -rf data/words.txt\n",
    "wrds.saveAsTextFile(\"data/words.txt\")\n",
    "wrds.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.0  as numbers\n",
      "5.0  as strings\n"
     ]
    }
   ],
   "source": [
    "numbs = spark.sparkContext.parallelize([1.0, 5.0, 43.0, 10.0])\n",
    "print(numbs.max(),\" as numbers\")\n",
    "print(numbs.max(key=str),\" as strings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Еще немного практики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "трансформация `glom()`\n",
    "\n",
    "Собирает все элементы раздела в список."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is'], ['spark', 'and'], ['it', 'is', 'great']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Кэширование**\n",
    "\n",
    "Как и с датафреймами - не сможем это эффективно использовать, но упомянуть стоит.\n",
    "\n",
    "`cache()` - кэширует RDD (после выполнения любого следующего действия), после этого RDD уже не будет каждый раз исполняться, значения будут браться из кэша. \n",
    "\n",
    "После выполнения этого узла можно будет увидеть статус RDD здесь - localhost:4040/storage/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds.cache()\n",
    "wrds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратная операция - `unpersist()`\n",
    "\n",
    "После ее выполнения кэш пропадет (см. localhost:4040/storage/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Использование RDD для небольших XML файлов**\n",
    "\n",
    "Как один из вариантов практического использования RDD - работа с мелкими XML файлами (помните, мы разбирали проблематику в модуле про работу с источниками?).\n",
    "\n",
    "Можно считать XML (в преобразованном, конечно, виде) в RDD, а дальше с помощью `map()` \"вытащить\" нужные поля в \"плоскую\" часть, оставив весь исходный XML в виде одного из полей. Потом эту \"регулярную\" структуру сохранить в реляционной таблице, например.\n",
    "\n",
    "Это будет не очень эффективно - см выше - но для обработки относительно небольшого потока XML может и хватить. \n",
    "\n",
    "Как мысль."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'pete', {'details': {'name': 'pete', 'phone': 123}, 'id': 1}),\n",
       " (2, 'mike', {'details': {'name': 'mike', 'phone': 999}, 'id': 2})]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dList = [ \n",
    "    { \"id\": 1, \"details\": { \"name\": \"pete\", \"phone\": 123 } },\n",
    "    { \"id\": 2, \"details\": { \"name\": \"mike\", \"phone\": 999 } },\n",
    "]\n",
    "dictRdd = spark.sparkContext.parallelize(dList)\n",
    "\n",
    "def mkRecord(el):\n",
    "    return ( el[\"id\"], el[\"details\"][\"name\"], el)\n",
    "\n",
    "dictRdd.map(mkRecord).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример вычисления числа Пи**\n",
    "\n",
    "Давайте, наконец, разберем - что же происходило в том примере, который мы использовали для вычисления числа Пи (в начале - когда тестировали работоспособность Spark), код приведен ниже.\n",
    "\n",
    "Теперь мы все знаем и можем объяснить - собственно, один из \"смыслов\" изучения RDD:\n",
    "\n",
    "* создается простейщий RDD (один раздел, список чисел нужного диапазона)\n",
    "* функция `filter()` позволяет нам выполнить код на питоне (вообще говоря - любой), в этом примере параметр (элемент RDD, для которого происходит вызов) не используется\n",
    "* функция `inside()` написана на python, использует его инструментарий\n",
    "* возвращает true или false, в зависимости от того, попали ли случайные числа в круг диаметром 2 или нет\n",
    "* фильтруя мы оставляем в RDD только \"попавшие\" в круг его элементы\n",
    "* `count()` посчитает число попавших в круг элементов\n",
    "* далее - простая математика (которая геометрия)\n",
    "\n",
    "Все просто! (\"как хорошо уметь читать...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1388\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_samples = 10000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
